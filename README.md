ANN : 인공신경망, 뉴런 네트워크에서 영감을 받은 머신러닝 모델
DNN : 심층신경망, 은닉층을 여러 개 쌓아 올린 인공 신경망
퍼셉트론 : 입력 값과 활성화 함수를 사용해 출력 값을 다음으로 넘기는 가장 작은 신경망 단위
활성화 함수 : 0 또는 1을 판단하는 함수
다층퍼셉트론 : 입력층과 출력층 사이에 은닉층을 만들어 좌표 평면을 왜곡시키는 단위
은닉층 : 가중치와 바이어스 값을 모아 한 번 더 시그모이드 함수를 이용해 최종 값으로 결과를 보냄
오차 역전파(backpropagation) : 임의의 초기 가중치를 준 뒤 결과를 계산하고 오차가 작아지는 방향으로 경사하강법을 이용해서 업데이트, 더이상 오차가 줄어들지 않을때까지 반복

활성화 함수
- 기울기 소실 : 층이 늘어나면서 기울기 값이 점점 작아져 맨 처음 층까지 전달되지 않는 문제
- 하이퍼볼릭 탄젠트(tanh) : 시그모이드 함수의 범위를 -1에서 1로 확장, 1보다 작은 값이 존재하므로 기울기 소실 문제가 사라지지 않음
- 렐루(ReLU) : 0보다 작으면 0, 0보다 큰 값은 그대로, 0보다 크기만하면 미분 값이 1이므로 은닉층을 거쳐도 맨 처음 층까지 살아남음
- 소프트플러스 : 렐루의 0이 되는 순간을 완화

고급 경사 하강법
- 확률적 경사 하강법(SGD) : 랜덤하게 추출한 일부 데이터를 사용하여 더 빨리 자주 업데이트, 진폭이 크고 불안정, 속도 개선
- 모멘텀(momentum) : 매번 기울기를 구하지만 오차를 수정하기 전 바로 앞 수정 값과 방향을 참고하여 같은 방향으로 일정한 비율만 수정, 정확도 개선
- 아다그라드(Adagrad) : 이동 보폭을 조절, 보폭 크기 개선
- 알엠에스프롭(RMSProp) : 아다그라드의 보폭 민감도를 보완, 보폭 크기 개선
- 아담(Adam) : 모멘텀과 알엠에스프롭 방법을 합친 방법, 정확도와 보폭 크기 개선, 현재 가장 많이 사용
- 
